{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the NLTK Book (Chapter 2)\n",
    "\n",
    "This notebook is based on the exercises described in [NLTK Book, Chapter 2: Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)\n",
    "<footer style=\"text-align:right;font-size:.8em;\">Source: Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. Oâ€™Reilly Media Inc. http://nltk.org/book</footer>\n",
    "\n",
    "\n",
    "* Corpora in NLTK\n",
    "* useful Python constructs\n",
    "* Some Python shortcuts\n",
    "\n",
    "Resources:\n",
    "* [NLTK documentation](https://www.nltk.org/)\n",
    "* [NLTK API Documentaiton](https://www.nltk.org/api/nltk.html)\n",
    "* [NLTK data and corpus available](http://www.nltk.org/nltk_data/)\n",
    "* [Corpus HOWTO at NLTK.org](http://www.nltk.org/howto/corpus.html)\n",
    "* [PyCharm](https://www.jetbrains.com/pycharm/download/#section=mac) An Integrated Data Environment (IDE) for working with Python. Has a greater focus on writing python code\n",
    "* [BootCat](https://bootcat.dipintra.it/)  Tool for gathering text from the internet.\n",
    "* [brat rapid annotation tool](http://brat.nlplab.org/index.html)\n",
    "* [Linguistic Data Consortium](https://www.ldc.upenn.edu/) A source for corpora\n",
    "* [European Language Resources Association](http://portal.elda.org/en/) Many useful language resource\n",
    "* [WordNet](https://wordnet.princeton.edu/) provides an onlie database for searching WordNet synsets.\n",
    "* [Global WordNet](http://globalwordnet.org/) provide information on and sharing of wordnet corpora in many languages.\n",
    "* [Ethnologue](https://www.ethnologue.com/) Information about every known living language of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gutneberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **corpus** is a large collection of text, texs, documents. \n",
    "\n",
    "[Project Gutneberg](https://www.gutenberg.org/) is a large collection (over 57,000) of digitized text for which copyright has expired, i.e. they are freely available.\n",
    "\n",
    "NLTK includes serveral corpus resources as part of its corpus module. \n",
    "To begin we will need to download the module. We will only download the guteneberg corpus but there are many more corpera in the corpus module. It is more common to only download the portions that are necessary. When we initiate the downloader you can see all the options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk and look at the gutneberg corpus texts\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access the gutenberg corpus. Observe that every text in the corpus has a `fileid`. We use this when we want to access the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to add a keyword for the corpus to make refering to it easier. \n",
    "# note that this may make it difficult for others to read when sharing notebooks.\n",
    "from nltk.corpus import gutenberg as gtb\n",
    "gtb.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's pick one of these texts and take a quick look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradise_lost = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "len(paradise_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradise_lost.concordance('alone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a difference between the corpus module and the book module that we used earlier. If we want to use the same functions we used in the book module we need to get the text into the correct data type. In this case `Text` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradise_lost.citation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(paradise_lost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the same viewer we used in the book module we need to transform the Gutenberg text into `Text` data type. Import the gutenberg corpus and transform it into an NLTK `text.Text` so we can work with is as we did the book collection in Chapter 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradise_lost_Text = nltk.Text(nltk.corpus.gutenberg.words('milton-paradise.txt'))\n",
    "paradise_lost_Text.concordance('alone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to work with a text from the gutneberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtb.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a short function to look at some of the characteristics of the gutenberg texts. This will show us the average word length, the average number of words per sentence, and how often words in the vocabulary are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gtb.fileids():\n",
    "    #the number of characters in a file\n",
    "    char = len(gtb.raw(file))\n",
    "    #the number of words in a file\n",
    "    words = len(gtb.words(file))\n",
    "    #the number of senteces in a file\n",
    "    sent = len(gtb.sents(file))\n",
    "    #the unique collection of words i.e. no repeats\n",
    "    vocab = len(set(w.lower() for w in gtb.words(file)))\n",
    "    print (file + '\\n  avg word length: ' + str(round(char/words)) + '\\n  avg sentence length: ' + str(round(words/sent)) + '\\n  times vocab used: ' + str(round(words/vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice raw() returns the untokenized version of the text. compare the two functions below\n",
    "print(gtb.raw('blake-poems.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gtb.words('blake-poems.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the longest sentence in a text:\n",
    "hamlet = gtb.sents('shakespeare-hamlet.txt')\n",
    "longest_sent = max(len(s) for s in hamlet)\n",
    "[s for s in hamlet if len(s) == longest_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtb.sents('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hamlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen the Gutenberg NLTK corpus reader has the following access methods: `raw()`, `words()` and `sents()`. Other corpa typically have these methods and may also provide more detailed access such as parts of speech access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web and Chat Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### web\n",
    "This corpus is less formal prose and more representative of common language usage. It is made up of content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, and wine reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grail = webtext.sents('grail.txt')\n",
    "print(grail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = webtext.raw('singles.txt')\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chat\n",
    "NLTK contains a corpus of instant messaging chat sessions \"originally collected by the Naval Postgraduate School for research on automatic detection of Internet predators.\" The corpus contains over 10,000 texts that have been anonamized. The fles contain chats from age specific chat rooms for a specific date. Some metadata is contained in the filename: date, chatroom, and number of posts (e.g. 10-19-20s_706posts.xml --> Date = 10-19, chatroom = 20's, number of posts = 706.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat\n",
    "for file in nps_chat.fileids():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = nps_chat.posts('10-19-adults_706posts.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 'U##' is the anonamized username."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown Corpus\n",
    "\"The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on.\" \n",
    "\n",
    "Sample:\n",
    "\n",
    "|ID |File\t|Genre\t|Description|\n",
    "|----|----|---------|---------|\n",
    "|A16\t|ca16\t|news\t|Chicago Tribune: Society Reportage|\n",
    "|B02\t|cb02\t|editorial\t|Christian Science Monitor: Editorials|\n",
    "|C17\t|cc17\t|reviews\t|Time Magazine: Reviews|\n",
    "|D12\t|cd12\t|religion\t|Underwood: Probing the Ethics of Realtors|\n",
    "|E36\t|ce36\t|hobbies\t|Norling: Renting a Car in Europe|\n",
    "\n",
    "For a complete genre list, see http://icame.uib.no/brown/bcm-los.html).\n",
    "\n",
    "For a more detailed overview [The Brown Corpus Manual](http://www.hit.uib.no/icame/brown/bcm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import the Brown Corpus and see what the categories are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a list of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a collection of texts based on the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery = brown.fileids(categories='mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the text of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_text=brown.words(categories='mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mystery_text))\n",
    "print(mystery_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to access the text in the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words(fileids=['cg17'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents(categories=['news', 'editorial', 'reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting exercise to do with this Corpus is to look at word use across different categories using Frequency Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fq_dist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "word_list = ['death','freedom','space','alien']\n",
    "for w in word_list:\n",
    "    print (w + ':',fq_dist[w], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good place to introduce Conditional Frequency Distribution. This will allow us to look at the distribution of words across corpus based on specific conditions. In this case the condition is that it be a part of one of the genre we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_fq_dist = nltk.ConditionalFreqDist((genre,word)\n",
    "                   for genre in brown.categories()\n",
    "                   for word in brown.words(categories=genre))\n",
    "genre_list=['science_fiction','news']\n",
    "word_list = ['alien','death','freedom']\n",
    "con_fq_dist.tabulate(conditions=genre_list,samples=word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Corpus\n",
    "\n",
    "This is a collection of news articles from Reuters. The collection documents have 90 classification topics and it is divided into test and training files for testing automatic document classification alogrithms. This begins to get into the area of machine learning and we will discuss this in more detail later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find texts based on category.\n",
    "reuters.fileids('yen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words('training/9946')[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.sents('training/9946')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inaugural Address Corpus\n",
    "\n",
    "This is a collection of 55 inaugaural addressed. This collection is interesting because it contains temporal data that can be used to look at changes in the text across time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the file name contains the year of the speech and the name of the president\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.words('2009-Obama.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The year of each text appears in the file ID. To get the year you must extract it from the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [file[:4] for file in inaugural.fileids()]\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Conditional Frequency Distribution again to get the freq distribution of particular words in each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the use of words in speeched over time.\n",
    "con_fq_dist = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "        for w in inaugural.words(fileid)\n",
    "            for target in ['war','freedom']\n",
    "            if w.lower().startswith(target))\n",
    "\n",
    "%matplotlib inline\n",
    "con_fq_dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your own corpus\n",
    "\n",
    "if you have a collection of files you can load it using the NLTK PlaintextCorpusReader. I have created a collection of text from the internet by doing a web scrape for pages about funny cat videos using [BootCat](https://bootcat.dipintra.it/). Import this corpus and use the tools we have looked at to explore what it might contain. Full disclosure: I have note looked at it closely so I cannot tell you anything about it either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "root = \"cat_corpus\"\n",
    "cats = PlaintextCorpusReader(root, '.*')\n",
    "cats.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.words('80.txt')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.sents('01.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try reusing code from earlier examinations and think about:\n",
    "\n",
    "* Is there anything you wish you had a function for?\n",
    "* What types of features are most useful?\n",
    "* Is there anything missing from the data you wish you had?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we adapt this code from the gutenberg corpus to give us a broad overview of this cat corpus.\n",
    "for file in gtb.fileids():\n",
    "    #the number of characters in a file\n",
    "    char = len(gtb.raw(file))\n",
    "    #the number of words in a file\n",
    "    words = len(gtb.words(file))\n",
    "    #the number of senteces in a file\n",
    "    sent = len(gtb.sents(file))\n",
    "    #the unique collection of words i.e. no repeats\n",
    "    vocab = len(set(w.lower() for w in gtb.words(file)))\n",
    "    print (file + '\\n  avg word length: ' + str(round(char/words)) + '\\n  avg sentence length: ' + str(round(words/sent)) + '\\n  times vocab used: ' + str(round(words/vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency Distributions\n",
    "\n",
    "We used the Conditional Frequency Distributions utility earlier. Let's go back to that and see how it works.\n",
    "\n",
    "This method of analysis generates a frequency distribution for a text that meets a certain criteria set out. With Conditional Frequency Distribution we can categorize and compare frequency distribution based on conditions we set out. For example, we could compare the frequency of color in different genre of text from the Brown Corpus; how often is color used in romance compared to science fiction.\n",
    "\n",
    "To begin we must understand that in conditional frequency we are not just comparing a list of words:\n",
    "\n",
    "`['red','green','blue','gold','yellow']`\n",
    "\n",
    "But we are looking at the frequency of a pair of terms, the condition and the term:\n",
    "\n",
    "`[('romance','red'),('romance','green'),('romance','blue'),('romance','gold'),('romance','yellow')]`\n",
    "\n",
    "Let's try this with the Browm Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word list based on our criteria of romance and science fiction:\n",
    "genre_wordlist = [(genre,word)\n",
    "                 for genre in ['romance', 'science_fiction']\n",
    "                 for word in brown.words(categories=genre)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the list we create.\n",
    "# notice that we have a list of tuples.\n",
    "genre_wordlist[:6], genre_wordlist[-6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now have each word paired with the category. Understandably this is a large list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(genre_wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `ConditionalFreqDist` to look at the difference in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show us the frequency distribution of all words in each genre we chose for our list\n",
    "cfd = nltk.ConditionalFreqDist(genre_wordlist)\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at each of the distributions\n",
    "print(cfd['romance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfd['science_fiction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['romance'].most_common(10),cfd['science_fiction'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably start making removing punctuation and stopwords a habit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot() and tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `plot()` we can plot out the frequency of a distribution on a graph. This is often useful when time is a a variable, as we did with the inaugural address. However, building on our previous example we can select some key words and plot the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dif = nltk.ConditionalFreqDist(\n",
    "    (genre,word)\n",
    "    for genre in ['romance','science_fiction']\n",
    "    for w in brown.words(categories=genre)\n",
    "    for word in ['red','gold','black','white','yellow'] if w.lower() == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(color_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dif.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent this Conditional Frequency Distribution in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dif.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And use keywords to refine the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dif.tabulate(conditions = ['romance','science_fiction'],samples=['black','white','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever wondered what category talks about what day mostr frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_days = nltk.ConditionalFreqDist(\n",
    "        (genre,word)\n",
    "        for genre in ['news','romance']\n",
    "        for word in brown.words(categories=genre))\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_days.tabulate(samples=days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "There is a corpus availael in nltk that is a collection of words and punctuation that occur with a high-frequency but do not affect meaning of a text. THese are words like 'a', 'the', 'but'. These stop words can often be filtered out of a text to remove 'noise' from some types of analysis, such as analysing subject matter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can see that stopwords are availabel in many languages.\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at a text with and without the stopwords and see how it impacts the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_distribution(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    count = [word for word in text if word.lower() in stopwords]\n",
    "    print ('Count of stopwords in text: ' + str(len(count)) + ' out of a total of ' + str(len(text)) + ' words')\n",
    "    print ('That is a text that is',(len(count)/len(text)*100),'% stopwords, not including punctuation.')\n",
    "    \n",
    "stopword_distribution(nltk.corpus.brown.words(categories='romance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names Corpus\n",
    "There is also a wordlist of 8,000 first names sorted by gender into separate files. You can use this to find names in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this we can see what names are shared by men and women\n",
    "from nltk.corpus import names\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at names that appear in both lists we can generate a list of names that are not gender specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nongendered_names = [name for name in names.words('male.txt') if name in names.words('female.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nongendered_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nongendered_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or look at a frequency distribution, is there a letter more common with men or women's name?\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (gender, name[0])\n",
    "    for gender in names.fileids()\n",
    "    for name in names.words(gender))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the counts are throwing this off. But we can still see some interesting facets of the data. And we could write a function to look at the percentages. (right?) There are simply more female names in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names.words('male.txt')), len(names.words('female.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_perc():\n",
    "    import string\n",
    "    letters = list((string.ascii_uppercase))\n",
    "    for l in letters:\n",
    "        ftotal = [n for n in names.words('female.txt') if n[0] == l]\n",
    "        mtotal = [n for n in names.words('male.txt') if n[0] == l]\n",
    "        fperc = len(ftotal)/len(names.words('female.txt')) * 100\n",
    "        mperc = len(mtotal)/len(names.words('male.txt')) * 100\n",
    "        print (l + '\\n   Female percent: ' + str(round(fperc,2)) + '\\n   Male percent:   ' + str(round(mperc,2)) +'\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_perc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look further into Machine Learning and Natural Language Processing, we will see how to establish more uniformity to test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "[WordNet](https://wordnet.princeton.edu/)\n",
    "\n",
    "WordNet is a large, English, lexical database that groups parts of speech into \"synsets\" (essentially groups of synonyms) that express a concept (e.g. \"bicycle\" and \"bike\"). NLTK provides access to the English WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give us the sysnset identification for the different concepts that \"\"bicycle\" is used to express.\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('bicycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have two possibilities. One bicycle as a noun (`bicycle.n.01` with the `n` denoting a noun) and a verb (`bicycle.v.01` with the `v` denoting a verb).\n",
    "\n",
    "Let's take a closet look at these. Using `lemma_names()` will give us the othe lemma that are assoociated with our term as synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at one synset\n",
    "wn.synset('bicycle.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show us all of the terms in all of the lemmas\n",
    "for lm in wn.synsets('bicycle'):\n",
    "    ls = lm.lemma_names()\n",
    "    lma = lm\n",
    "    print (str(lma) + ': ' + str(ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a definition of the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('bicycle.n.01').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some concepts even have examples of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also identify the words by the lemma to eliminate abiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will show us all the lemmas in the synset bicycle.n.01 \n",
    "wn.synset('bicycle.v.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemma('bicycle.n.01.bike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may seem redundant to us but it is useful for a computer. \n",
    "# we can get the lemma from one of its inflected forms.\n",
    "wn.lemma('bicycle.n.01.bike').synset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can get all of the lemmas that contain a selected word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemmas('bike')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchies\n",
    "\n",
    "With WordNet synsets we can also look at associations between concepts. Some basic concpets are broad and make up the root, but from that root it is possible to navigate to more specific instances of the root's broader concept.\n",
    "\n",
    "Let's look at some of the more specific instances of the root concept, the **hyponyms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike = wn.synset('bike.n.01')\n",
    "types_of_bikes = bike.hyponyms()\n",
    "types_of_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think of this constructor this way --> `wn.synset('minibike.n.01').lemmas()[0].name()`\n",
    "\n",
    "sorted(lemma.name() for synset in types_of_bikes for lemma in synset.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_bikes[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look up the hierarchy at hypernyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = bike.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks at one of the two paths to reach our original synset bike.n.01\n",
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can see the root hypernyms for all the paths\n",
    "bike.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another relationship that can be explored between synsets are the `meronyms` and `holonyms`. `Meronyms` are componets of the term, and `holonyms` are things that contain the term. There are substance and parts for these two lexical relations. a substance consist of what it is made up of, while a part are sections or elements of the term. For example, a part meronym for \"bike\" is \"kick stand\". A tree's substance is the type of wood it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show all of the meronyms of the \"bike.n.01\" synset\n",
    "bike.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bike has no holonyms so let's look at a tree. \n",
    "wn.synsets('tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can become complicated when diferent synsets of the same word begin to relate to other.\n",
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "    print(synset.name() + ':', synset.definition())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('mint.n.04').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('mint.n.04').substance_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of relationship exists between verbs, **entailments**. The act of eating involves the act of chewing so the verb \"eat\" entails the verb \"chew\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another relationship between synset lemmas is **antonymy**, or antonyms. These are lemmas with a meaning counter to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('hard.a.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemma('hard.a.02.hard').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
