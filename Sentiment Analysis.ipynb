{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer\n",
    "\n",
    "Understand how positive or negative a text is. This is usefule for understanding reviews, tweets, and comments. \n",
    "\n",
    "The data:<br/>\n",
    "[link to Multi-Domain Sentiment Dataset](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html)\n",
    "\n",
    "## About the data\n",
    "This sentiment dataset was used in our paper:\n",
    "\n",
    "John Blitzer, Mark Dredze, Fernando Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. Association of Computational Linguistics (ACL), 2007. [[PDF](http://www.cs.jhu.edu/~mdredze/publications/sentiment_acl07.pdf)]\n",
    "\n",
    "\n",
    "If you use this data for your research or a publication, please cite the above paper as the reference for the data. Also, please drop me a line so I know that you found the data useful.\n",
    "\n",
    "\n",
    "The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from 4 product types (domains): Kitchen, Books, DVDs, and Electronics. Each domain has several thousand reviews, but the exact number varies by domain. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed. This page contains some descriptions about the data. If you have questions, please email me directly ([email found here](http://www.cs.jhu.edu/~mdredze/)).\n",
    "\n",
    "\n",
    "A few notes regarding the data.\n",
    "\n",
    "\n",
    "1) There are 4 directories corresponding to each of the four domains. Each directory contains 3 files called positive.review, negative.review and unlabeled.review. (The books directory doesn't contain the unlabeled but the link is below.) While the positive and negative files contain positive and negative reviews, these aren't necessarily the splits we used in the experiments. We randomly drew from the three files ignoring the file names.\n",
    "\n",
    "\n",
    "2) Each file contains a pseudo XML scheme for encoding the reviews. Most of the fields are self explanatory. The reviews have a unique ID field that isn't very unique. If it has two unique id fields, ignore the one containing only a number.\n",
    "\n",
    "## The Plan\n",
    "* Examine the electronics category (but the same code can be applied to others\n",
    "* Do a classification on positive or negative because these reviews are already classified (we will ignore the starred ratings)\n",
    "* The files are in XML so we will need a parser (Beautiful Soup)\n",
    "* We will need 2 passes. One to determine vocabulary size (and remove stop words) and one to create vectors.\n",
    "* We will use a classifier model because this is a classification problem.\n",
    "* And we'll use logistic regression so we can interpret the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The factor we need to remember when looking at the logistic regression is that the weights of the linear equation indicate the impact of a feature on the outcome. Positive coefficients have a positive impact on the outcome while negative coefficients have a negative impact. coefficents of zero or close to zero have none to little impact on the output. The logisitc regression can also give us an idea to the level of certainty so we can identify the certainty of a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages and libraries\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will turn words in to their base word (e.g. cas ==> cat)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat our list of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the positive reviews\n",
    "positive_reviews = bs(open('data/sentiment/electronics/positive.review').read(),'lxml')\n",
    "# take a peek at the xml and you can see we are only looking for the element <review_text>\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the positive reviews\n",
    "negative_reviews = bs(open('data/sentiment/electronics/negative.review').read(),'lxml')\n",
    "# take a peek at the xml and you can see we are only looking for the element <review_text>\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num positive reviews:  1000 \n",
      "Num negative reviews:  1000\n"
     ]
    }
   ],
   "source": [
    "#make sure the datasets are the same size\n",
    "print('Num positive reviews: ',len(negative_reviews), '\\nNum negative reviews: ', len(positive_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If the data sets were not the same size we would want to shuffle the larger set and then remove the extra rows.\n",
    "\n",
    "`np.random.shuffle(larger_set)\n",
    "larger_set = larger_set[:len(smaller_set)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index for each of the words in the documents.\n",
    "# how many words are in the documents\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    #tokenize the string\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    # remove words of 1 or 2 characters (e.g. \"'s\", \"an\", \"it\")\n",
    "    tokens = [t for t in tokens if len(t)>2]\n",
    "    # get the token lemmas\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "    \n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] =current_index\n",
    "            current_index +=1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] =current_index\n",
    "            current_index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the array with the lables and the features\n",
    "def tokens_to_vector(tokens,label):\n",
    "    x = np.zeros(len(word_index_map) + 1)\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x/x.sum()\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idenitfying the rows or N factor of the NxD array (i.e. dimension)\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "# creating the array to hols the data\n",
    "data = np.zeros((N,(len(word_index_map) + 1)))\n",
    "i = 0\n",
    "\n",
    "# put the tokens in the array\n",
    "\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens,1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens,0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "# shuffle the data because right now all of the positive reviews are stacked on the negative\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the array into features and labels\n",
    "X = data[:,:-1]\n",
    "Y = np.ravel(data[:,-1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X_train = X[:-100,]\n",
    "Y_train = Y[:-100,]\n",
    "x_test = X[-100:,]\n",
    "y_test = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate:  0.73\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "print(\"Classification rate: \", model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit :  -0.5395088250463969\n",
      "bad :  -0.6040821789820418\n",
      "cable :  0.6806291275119105\n",
      "time :  -0.713272987457447\n",
      "used :  1.014175690767491\n",
      "'ve :  0.520810917731183\n",
      "month :  -0.6286165311318604\n",
      "problem :  0.5584639450917689\n",
      "need :  0.5955134261283141\n",
      "good :  1.7288606307595995\n",
      "sound :  1.0225085054263092\n",
      "like :  0.578202625980877\n",
      "lot :  0.5464679990212311\n",
      "n't :  -1.8104674658150908\n",
      "easy :  1.3634575382191199\n",
      "get :  -1.0899412851930284\n",
      "use :  1.6323682100032912\n",
      "quality :  1.3135207212237405\n",
      "best :  0.9044055154547704\n",
      "item :  -0.848394414107202\n",
      "working :  -0.5193466604828412\n",
      "well :  0.9091145116394838\n",
      "wa :  -1.215304786165066\n",
      "perfect :  0.878869735019799\n",
      "fast :  0.7766885555653344\n",
      "ha :  0.636487413408659\n",
      "price :  2.228570260288225\n",
      "great :  3.4794455537548425\n",
      "money :  -0.8746184448398857\n",
      "memory :  0.8080703581303034\n",
      "would :  -0.79392362629664\n",
      "buy :  -0.9523372600784666\n",
      "worked :  -0.7899525296038465\n",
      "pretty :  0.509938066000245\n",
      "could :  -0.5572956812832564\n",
      "doe :  -0.9746099635138824\n",
      "two :  -0.5730386388883612\n",
      "highly :  0.8513181533464518\n",
      "recommend :  0.5574243693817915\n",
      "first :  -0.6629101673285784\n",
      "customer :  -0.5450465704327988\n",
      "support :  -0.8096539306475357\n",
      "little :  0.6744405910124773\n",
      "returned :  -0.5954020682631332\n",
      "excellent :  1.1923595447741075\n",
      "love :  0.9739658826529494\n",
      "small :  0.5972885973579898\n",
      "got :  -0.5162550743136155\n",
      "week :  -0.5617688796604277\n",
      "using :  0.5080411865660341\n",
      "thing :  -0.7839386000509553\n",
      "even :  -0.743424704379919\n",
      "poor :  -0.6911704005797975\n",
      "tried :  -0.6875122721868236\n",
      "back :  -1.4799396175659691\n",
      "try :  -0.5387230101637432\n",
      "comfortable :  0.550288945864811\n",
      "speaker :  0.8085117629760852\n",
      "warranty :  -0.5068839873187673\n",
      "paper :  0.5047476360295137\n",
      "return :  -0.9519414373291167\n",
      "waste :  -0.841099632876158\n",
      "refund :  -0.526430911481507\n"
     ]
    }
   ],
   "source": [
    "# Now we can look at the weights and see which words are weighted heavier than others.\n",
    "\n",
    "threshold = .5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print (word, ': ', weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
