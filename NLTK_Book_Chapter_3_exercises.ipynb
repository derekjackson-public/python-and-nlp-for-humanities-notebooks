{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the NLTK Book (Chapter 3)\n",
    "[NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "Resources:\n",
    "* [urllib](https://docs.python.org/3/library/urllib.html) <br/>Python package for working with urls.\n",
    "* [Regular Expression module](https://docs.python.org/3/library/re.html) <br/>allows us to [use regular expressions in python](https://docs.python.org/3/howto/regex.html#regex-howto) strings\n",
    "* [Data pretty printer](https://docs.python.org/3/library/pprint.html) <br/>print data structures in a readable format\n",
    "* [Project Guttenberg catalog](http://www.gutenberg.org/catalog/)<br/>find electronice texts from Project Guttenberg's collection that are not inlcuded in NLTK.\n",
    "* [textfiles.com](http://www.textfiles.com/directory.html) <br/>A usefule source for finding plain text files.\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) <br/>A Python library that helps us work with HTML and XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib import request\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the text\n",
    "Find a text from the Project Guttenberg colleciton or from textfile.com using urllib. You should browse the website to get the url you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/cache/epub/7178/pg7178.txt'\n",
    "response = request.urlopen(url)\n",
    "raw_text = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just retrieved the text for Marcel Proust's 'Swann's Way' from the Project Guttenberg catalog and turned into plain text (i.e. a string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will tell us how many characters (not words) long the text is. \n",
    "# In order to get a word count we need to do some processing to this text.\n",
    "\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Turning the text into words using the nltk word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_text = word_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a list and not a string.\n",
    "# The list contains the words if the text as identified by the word_tokenizer\n",
    "type(words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can get a better approximation of the word count\n",
    "len(words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take our tokeinzed text, the list of strings and turn it into an NLTK Text and carry out all of the processing we saw earlier. (e.g. collocation, similar, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we turn the list into an nltk text.\n",
    "nltk_text = nltk.Text(words_text)\n",
    "type(nltk_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('blue',lines=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.similar('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red','blue','green','black','white']\n",
    "nltk_text.dispersion_plot(colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text(nltk_text[100000:200000]).concordance('blue',width=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "freqDist = FreqDist(nltk_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist['blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist.hapaxes()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Grooming' or 'munging' the text file\n",
    "\n",
    "We can see that bigrams like \"Project Guttneberg\" and \"Archive Foundation\" and \"electronic works\" appear frequently in the text. Let's take a closer look and see if we want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go back to the raw text data so that we can create a clean NLTK text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:1000]\n",
    "#let's look for the start of the book (hint: Project Guttenberg gives us some clues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the actual text of the book book starts with the text  \"OVERTURE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can find where the string starts\n",
    "raw_text.find('OVERTURE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[842]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Now where does it end? <br/>\n",
    "Warning: Project Guttneberg has a long 'footer' but Project Guttenberg is good about helping out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[-20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text.rfind('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nEnd of the Project Gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[1102563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"hello\"\n",
    "string = string[1:3]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = raw_text[842:1102537]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily rebuild our NLTK Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = nltk.Text(word_tokenize(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with HTML\n",
    "\n",
    "A lot of modern text and language we encounter now is online and not necessarily presented in a text file. Getting documents from the web will likely require working with html documents. The initial process is the same. Find a url from a web page that you might like to take a closer look at. The easiest place to start is a news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is the second time we have done this so we might want to think about turning this into a function.\n",
    "\n",
    "def get_text(url):\n",
    "    response = request.urlopen(url)\n",
    "    text = response.read().decode('utf8')\n",
    "    return text\n",
    "html = get_text('https://www.bbc.com/news/entertainment-arts-45818204')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a string of text that has html markup\n",
    "html[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where BeautifulSoup comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will give us a warning. If we want to avoid the warning we can pass in the parser we would like to use.\n",
    "# bydefault the parser is pythons lxml parser. This is what we want to use so we can ignore the warning.\n",
    "soup = bs(html,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's trim down the HTML to just the bit we want. If we use the `get_text() function` we will also get a lot of text we don't want (like the javascript inside the `script>` tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's take a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what is in the <body> element.\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If this is well written HTML there should be on <h1> (and we will be lucky)\n",
    "soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic HTML (Yeah!) will often use the '<article>' tag to hold main content.\n",
    "print(soup.find('article'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in soup.h1.children:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = soup.h1.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = body.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx in article:\n",
    "    print(tx.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not bad but...\n",
    "This is close but if we really want the article we can do a little better.\n",
    "\n",
    "Looking at the html it looks like the main article is in a `<div>` tag with a class of \"story-body_inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div',{\"class\":\"story-body__inner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find('div',{\"class\":\"story-body__inner\"}).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is getting better, we have the right section so now let's just get the `<p>` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = soup.find('div',{\"class\":\"story-body__inner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in div.find_all('p'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like this is the one. TO save it as a string of text we will need to create a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(ls):\n",
    "    words = \"\"\n",
    "    for el in ls:\n",
    "        words = words + ' ' + el.text # if we want to keep the paragrapgh structure we can add `+'\\n'`\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = get_article(div.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px\"/>\n",
    "Now we can tokenize and tokenize and process the article as we did the previous text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with local files \n",
    "In some cases we might have files already availale to us that we want to examine. To access local files we can use python to load and read any local documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see what files are in our current directory \n",
    "# (on windows these commands are different but the process is the same.)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls cat_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `os` python module to examine files in directories before using the pythons `read` and `open` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('cat_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cat_corpus/00.txt')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see we have a string ready to process\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for every new line there is a \"\\n\" which is the notation for a new line in a text file. We can filter this out if we do not need to preserve that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cat_corpus/00.txt')\n",
    "new_text = \"\"\n",
    "for line in file:\n",
    "    new_text = new_text + (line.strip('\\n'))\n",
    "    \n",
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helpful string methods\n",
    "\n",
    "|Method\t|Functionality|\n",
    "|-------|-------------|\n",
    "|s.find(t)|\tindex of first instance of string t inside s (-1 if not found)|\n",
    "|s.rfind(t)|\tindex of last instance of string t inside s (-1 if not found)|\n",
    "|s.index(t)\t|like s.find(t) except it raises ValueError if not found|\n",
    "|s.rindex(t)|\tlike s.rfind(t) except it raises ValueError if not found|\n",
    "|s.join(text)|\tcombine the words of the text into a string using s as the glue|\n",
    "|s.split(t)\t|split s into a list wherever a t is found (whitespace by default)|\n",
    "|s.splitlines()|\tsplit s into a list of strings, one per line|\n",
    "|s.lower()\t|a lowercased version of the string s|\n",
    "|s.upper()\t|an uppercased version of the string s|\n",
    "|s.title()\t|a titlecased version of the string s|\n",
    "|s.strip()\t|a copy of s without leading or trailing whitespace|\n",
    "|s.replace(t, u)|\treplace instances of t with u inside s|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"WE went to the Supermarlet on Wednesday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.replace('marlet','market')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"-\".join(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helpful Regular expression patterns\n",
    "<table border=\"1\" class=\"docutils\" id=\"tab-regexp-meta-characters1\">\n",
    "<colgroup>\n",
    "<col width=\"15%\">\n",
    "<col width=\"85%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr><th class=\"head\">Operator</th>\n",
    "<th class=\"head\">Behavior</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">.</span></tt></td>\n",
    "<td>Wildcard, matches any character</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">^abc</span></tt></td>\n",
    "<td>Matches some pattern <span class=\"math\">abc</span> at the start of a string</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">abc$</span></tt></td>\n",
    "<td>Matches some pattern <span class=\"math\">abc</span> at the end of a string</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">[abc]</span></tt></td>\n",
    "<td>Matches one of a set of characters</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">[A-Z0-9]</span></tt></td>\n",
    "<td>Matches one of a range of characters</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">ed|ing|s</span></tt></td>\n",
    "<td>Matches one of the specified strings (disjunction)</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">*</span></tt></td>\n",
    "<td>Zero or more of previous item, e.g. <tt class=\"doctest\"><span class=\"pre\">a*</span></tt>, <tt class=\"doctest\"><span class=\"pre\">[a-z]*</span></tt> (also known as <em>Kleene Closure</em>)</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">+</span></tt></td>\n",
    "<td>One or more of previous item, e.g. <tt class=\"doctest\"><span class=\"pre\">a+</span></tt>, <tt class=\"doctest\"><span class=\"pre\">[a-z]+</span></tt></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">?</span></tt></td>\n",
    "<td>Zero or one of the previous item (i.e. optional), e.g. <tt class=\"doctest\"><span class=\"pre\">a?</span></tt>, <tt class=\"doctest\"><span class=\"pre\">[a-z]?</span></tt></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">{n}</span></tt></td>\n",
    "<td>Exactly <span class=\"math\">n</span> repeats where n is a non-negative integer</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">{n,}</span></tt></td>\n",
    "<td>At least <span class=\"math\">n</span> repeats</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">{,n}</span></tt></td>\n",
    "<td>No more than <span class=\"math\">n</span> repeats</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">{m,n}</span></tt></td>\n",
    "<td>At least <span class=\"math\">m</span> and no more than <span class=\"math\">n</span> repeats</td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">a(b|c)+</span></tt></td>\n",
    "<td>Parentheses that indicate the scope of the operators</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.corpus.words.words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find words by the ending\n",
    "[w for w in words if re.search('ing$', w)][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wildcard is a well know search symbol: '.'\n",
    "[w for w in words if re.search('....eding', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can combine the regula expressions to find more complex patterns\n",
    "[w for w in words if re.search('ing.$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the '^' symbol to match the start of a word\n",
    "[w for w in words if re.search('^pre..ing', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the '[]' can be used to denote ranges\n",
    "[w for w in words if re.search('^[t-x][uy]', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the plus symbol '+' is repeating character of one or more.\n",
    "[w for w in words if re.search('s+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the star '*' means zero or more instances of the preceding character or set.\n",
    "# all the words ending in 'ing'\n",
    "[w for w in words if re.search('.*ing$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Text\n",
    "Let's look at normalizing text by tokenizing, stemming, and lemmatization. For the purposes of demonstration let's take a small bit of text so we can observe the changes we make. But these operations can be applied equally to large texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = 'NARRATOR : Sir Launcelot had saved Sir Galahad from almost certain temptation, but they were still no nearer the Grail. Meanwhile, King Arthur and Sir Bedevere, not more than a swallow\\'s flight away, had discovered something. Oh, that\\'s an unladen swallow\\'s flight, obviously. I mean, they were more than two laden swallows\\' flights away -- four, really, if they had a coconut on a line between them.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin let's tokenize the text so we can work with the individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = word_tokenize(raw_text)\n",
    "token_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides stemmers so we do not need to create an algorithm to create our own. Two stemmers available on Pyhton are the Porter and the Lancaster stemmers. They handle the stemming a little differently and will provide different results based on their algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can normalize text by making everything lower case.\n",
    "token_text_lower = [w.lower() for w in token_text]\n",
    "print(token_text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Stemmers have the normalization as part of the process\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = [porter.stem(t) for t in token_text]\n",
    "print(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan = [lancaster.stem(t) for t in token_text]\n",
    "print(lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(set(port).difference(set(lan))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(set(lan).difference(set(port))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is beyond the scope of our workshop but it is here to demonstrate some of the utility of the stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = book.text1\n",
    "porter = nltk.PorterStemmer()\n",
    "indexed_text = IndexedText(porter, text)\n",
    "indexed_text.concordance('die')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "NLTK relies on the WordNet lemmatizer to lemmatize words and only affects words that are in its dictionary. <br/>\n",
    "**Note:** The WordNet lemmatizer will convert 'women' to 'woman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnlemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'women' is in the dictionary and is returned as woman, but 'running' is not.\n",
    "[wnlemma.lemmatize(w) for w in ['women','running','children']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = book.text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_text = [wnlemma.lemmatize(w) for w in text]\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist = FreqDist(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
